{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f56a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [00:01<03:39,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5, Reward: 17.1293\n",
      "Policy Loss: -0.0118, Value Loss: 0.0716, Entropy Loss: -88.6023\n",
      "Current Intensity: 1.7904, Max Intensity: 1.7904\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [00:03<03:41,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Reward: 15.2031\n",
      "Policy Loss: -0.2264, Value Loss: 0.0446, Entropy Loss: -88.5742\n",
      "Current Intensity: 2.2056, Max Intensity: 2.2056\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/500 [00:05<04:08,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15, Reward: 15.4198\n",
      "Policy Loss: -0.0277, Value Loss: 0.0574, Entropy Loss: -88.5366\n",
      "Current Intensity: 2.2647, Max Intensity: 2.2647\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 20/500 [00:07<03:37,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20, Reward: 17.0469\n",
      "Policy Loss: -0.1616, Value Loss: 0.0583, Entropy Loss: -88.5086\n",
      "Current Intensity: 2.1818, Max Intensity: 2.1818\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 24/500 [00:08<02:14,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25, Reward: 17.9176\n",
      "Policy Loss: -0.0926, Value Loss: 0.0572, Entropy Loss: -88.4832\n",
      "Current Intensity: 2.3377, Max Intensity: 2.8476\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 25/500 [00:10<06:39,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 25: Average Reward = 21.7776\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 30/500 [00:13<05:28,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30, Reward: 18.3720\n",
      "Policy Loss: -0.0705, Value Loss: 0.0535, Entropy Loss: -88.4475\n",
      "Current Intensity: 2.8325, Max Intensity: 2.8325\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 35/500 [00:15<04:59,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35, Reward: 18.6310\n",
      "Policy Loss: -0.1603, Value Loss: 0.0389, Entropy Loss: -88.4033\n",
      "Current Intensity: 2.7246, Max Intensity: 2.7246\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 40/500 [00:18<04:47,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40, Reward: 16.4556\n",
      "Policy Loss: -0.1825, Value Loss: 0.0380, Entropy Loss: -88.4088\n",
      "Current Intensity: 2.7023, Max Intensity: 2.7023\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 45/500 [00:20<04:14,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45, Reward: 19.5596\n",
      "Policy Loss: -0.1621, Value Loss: 0.0642, Entropy Loss: -88.3584\n",
      "Current Intensity: 2.8558, Max Intensity: 2.8558\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 49/500 [00:21<02:44,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Reward: 19.8273\n",
      "Policy Loss: -0.1142, Value Loss: 0.0493, Entropy Loss: -88.3643\n",
      "Current Intensity: 2.8833, Max Intensity: 2.8833\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 50/500 [00:24<08:05,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 50: Average Reward = 23.4539\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 55/500 [00:26<04:41,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 55, Reward: 15.9919\n",
      "Policy Loss: -0.1915, Value Loss: 0.0382, Entropy Loss: -88.3165\n",
      "Current Intensity: 2.7465, Max Intensity: 2.7465\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 60/500 [00:29<04:22,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60, Reward: 19.0705\n",
      "Policy Loss: -0.2364, Value Loss: 0.0425, Entropy Loss: -88.2977\n",
      "Current Intensity: 2.8145, Max Intensity: 2.8145\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 65/500 [00:31<03:52,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 65, Reward: 17.2090\n",
      "Policy Loss: -0.2152, Value Loss: 0.0374, Entropy Loss: -88.2656\n",
      "Current Intensity: 2.8400, Max Intensity: 2.8400\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 70/500 [00:33<04:32,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70, Reward: 20.0959\n",
      "Policy Loss: -0.1173, Value Loss: 0.0473, Entropy Loss: -88.2551\n",
      "Current Intensity: 2.8135, Max Intensity: 2.8135\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 74/500 [00:34<02:34,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 75, Reward: 17.3560\n",
      "Policy Loss: -0.1285, Value Loss: 0.0361, Entropy Loss: -88.2160\n",
      "Current Intensity: 2.5865, Max Intensity: 2.5865\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 75/500 [00:37<07:05,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 75: Average Reward = 46.5794\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 80/500 [00:39<04:18,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80, Reward: 19.2606\n",
      "Policy Loss: -0.2288, Value Loss: 0.0350, Entropy Loss: -88.1267\n",
      "Current Intensity: 2.8495, Max Intensity: 2.8495\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 85/500 [00:41<04:06,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 85, Reward: 16.9741\n",
      "Policy Loss: -0.2844, Value Loss: 0.0358, Entropy Loss: -88.0909\n",
      "Current Intensity: 2.7527, Max Intensity: 2.7527\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 90/500 [00:44<04:09,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90, Reward: 18.0818\n",
      "Policy Loss: -0.1039, Value Loss: 0.0412, Entropy Loss: -88.0575\n",
      "Current Intensity: 2.9149, Max Intensity: 2.9149\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 95/500 [00:46<03:49,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 95, Reward: 17.0100\n",
      "Policy Loss: -0.0623, Value Loss: 0.0378, Entropy Loss: -88.0281\n",
      "Current Intensity: 2.8956, Max Intensity: 2.8956\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 99/500 [00:47<02:34,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Reward: 17.2791\n",
      "Policy Loss: -0.2186, Value Loss: 0.0328, Entropy Loss: -88.0239\n",
      "Current Intensity: 2.8494, Max Intensity: 2.8494\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [00:50<06:57,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 100: Average Reward = 50.2470\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 105/500 [00:52<04:29,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 105, Reward: 20.0831\n",
      "Policy Loss: -0.1104, Value Loss: 0.0427, Entropy Loss: -88.0404\n",
      "Current Intensity: 2.7885, Max Intensity: 2.7885\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 110/500 [00:55<03:54,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 110, Reward: 17.4164\n",
      "Policy Loss: -0.0802, Value Loss: 0.0235, Entropy Loss: -88.0415\n",
      "Current Intensity: 2.8481, Max Intensity: 2.8481\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 115/500 [00:57<03:38,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 115, Reward: 17.8198\n",
      "Policy Loss: -0.3242, Value Loss: 0.0401, Entropy Loss: -87.9797\n",
      "Current Intensity: 2.8215, Max Intensity: 2.8215\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 120/500 [00:59<04:01,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 120, Reward: 17.8801\n",
      "Policy Loss: -0.2460, Value Loss: 0.0310, Entropy Loss: -87.9351\n",
      "Current Intensity: 2.8322, Max Intensity: 2.8322\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 124/500 [01:01<02:26,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 125, Reward: 19.3985\n",
      "Policy Loss: -0.0360, Value Loss: 0.0369, Entropy Loss: -87.8049\n",
      "Current Intensity: 3.0190, Max Intensity: 3.0190\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 125/500 [01:03<06:33,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 125: Average Reward = 48.3532\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 130/500 [01:06<03:57,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 130, Reward: 18.6569\n",
      "Policy Loss: -0.1291, Value Loss: 0.0464, Entropy Loss: -87.7052\n",
      "Current Intensity: 2.9539, Max Intensity: 2.9539\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 135/500 [01:08<03:40,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 135, Reward: 18.0965\n",
      "Policy Loss: -0.1492, Value Loss: 0.0305, Entropy Loss: -87.7103\n",
      "Current Intensity: 3.6703, Max Intensity: 3.6703\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 140/500 [01:10<03:28,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 140, Reward: 20.2840\n",
      "Policy Loss: -0.1628, Value Loss: 0.0411, Entropy Loss: -87.6336\n",
      "Current Intensity: 3.5825, Max Intensity: 3.5825\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 145/500 [01:13<03:32,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 145, Reward: 19.1486\n",
      "Policy Loss: -0.2094, Value Loss: 0.0307, Entropy Loss: -87.6304\n",
      "Current Intensity: 3.6275, Max Intensity: 3.6275\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 149/500 [01:14<02:13,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150, Reward: 19.4361\n",
      "Policy Loss: -0.1325, Value Loss: 0.0311, Entropy Loss: -87.5797\n",
      "Current Intensity: 3.6144, Max Intensity: 3.6144\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 150/500 [01:17<06:04,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 150: Average Reward = 56.1487\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 155/500 [01:19<03:46,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 155, Reward: 20.4945\n",
      "Policy Loss: -0.2447, Value Loss: 0.0511, Entropy Loss: -87.5118\n",
      "Current Intensity: 3.0174, Max Intensity: 3.0174\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 160/500 [01:21<03:19,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 160, Reward: 18.2292\n",
      "Policy Loss: -0.0624, Value Loss: 0.0356, Entropy Loss: -87.4259\n",
      "Current Intensity: 3.1203, Max Intensity: 3.1203\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 165/500 [01:24<03:17,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 165, Reward: 22.1879\n",
      "Policy Loss: -0.2744, Value Loss: 0.0515, Entropy Loss: -87.2120\n",
      "Current Intensity: 3.0565, Max Intensity: 3.0565\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 170/500 [01:26<03:40,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 170, Reward: 17.3716\n",
      "Policy Loss: -0.1785, Value Loss: 0.0343, Entropy Loss: -87.2152\n",
      "Current Intensity: 3.0466, Max Intensity: 3.0466\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 174/500 [01:27<02:12,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 175, Reward: 19.2408\n",
      "Policy Loss: -0.2110, Value Loss: 0.0372, Entropy Loss: -87.0809\n",
      "Current Intensity: 2.9347, Max Intensity: 2.9347\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 175/500 [01:30<05:44,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 175: Average Reward = 52.4522\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 180/500 [01:32<03:32,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 180, Reward: 18.7881\n",
      "Policy Loss: -0.0715, Value Loss: 0.0394, Entropy Loss: -86.9975\n",
      "Current Intensity: 3.4527, Max Intensity: 3.4527\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 185/500 [01:35<03:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 185, Reward: 20.5636\n",
      "Policy Loss: -0.2174, Value Loss: 0.0434, Entropy Loss: -87.0756\n",
      "Current Intensity: 3.3348, Max Intensity: 3.3348\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 190/500 [01:37<03:04,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 190, Reward: 19.4714\n",
      "Policy Loss: -0.2501, Value Loss: 0.0299, Entropy Loss: -87.0197\n",
      "Current Intensity: 3.4182, Max Intensity: 3.4182\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 195/500 [01:40<03:05,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 195, Reward: 18.4446\n",
      "Policy Loss: -0.1120, Value Loss: 0.0304, Entropy Loss: -86.8039\n",
      "Current Intensity: 3.4497, Max Intensity: 3.4497\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 199/500 [01:41<01:58,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Reward: 21.5685\n",
      "Policy Loss: -0.1236, Value Loss: 0.0328, Entropy Loss: -86.7445\n",
      "Current Intensity: 3.0586, Max Intensity: 3.0586\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [01:45<07:19,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 200: Average Reward = 61.1654\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 205/500 [01:48<03:45,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 205, Reward: 22.9438\n",
      "Policy Loss: -0.1974, Value Loss: 0.0396, Entropy Loss: -86.7450\n",
      "Current Intensity: 3.7045, Max Intensity: 3.7045\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 210/500 [01:50<02:48,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 210, Reward: 18.5450\n",
      "Policy Loss: -0.2396, Value Loss: 0.0286, Entropy Loss: -86.7035\n",
      "Current Intensity: 3.5418, Max Intensity: 3.5418\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 215/500 [01:52<03:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 215, Reward: 22.3000\n",
      "Policy Loss: -0.2115, Value Loss: 0.0444, Entropy Loss: -86.7188\n",
      "Current Intensity: 3.5930, Max Intensity: 3.5930\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 220/500 [01:55<02:41,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 220, Reward: 18.1943\n",
      "Policy Loss: -0.1526, Value Loss: 0.0379, Entropy Loss: -86.6723\n",
      "Current Intensity: 3.6321, Max Intensity: 3.6321\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 224/500 [01:56<01:38,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 225, Reward: 17.7896\n",
      "Policy Loss: -0.1334, Value Loss: 0.0415, Entropy Loss: -86.7454\n",
      "Current Intensity: 3.6285, Max Intensity: 3.6285\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 225/500 [01:58<04:57,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 225: Average Reward = 66.3391\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 230/500 [02:01<02:48,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 230, Reward: 19.9874\n",
      "Policy Loss: -0.0959, Value Loss: 0.0377, Entropy Loss: -86.6095\n",
      "Current Intensity: 3.6557, Max Intensity: 3.6557\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 235/500 [02:03<02:47,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 235, Reward: 16.6955\n",
      "Policy Loss: -0.0819, Value Loss: 0.0464, Entropy Loss: -86.5739\n",
      "Current Intensity: 3.8413, Max Intensity: 3.8413\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 240/500 [02:05<02:22,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 240, Reward: 20.2072\n",
      "Policy Loss: -0.1127, Value Loss: 0.0403, Entropy Loss: -86.7493\n",
      "Current Intensity: 3.8070, Max Intensity: 3.8070\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 245/500 [02:08<02:33,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 245, Reward: 19.8992\n",
      "Policy Loss: -0.0482, Value Loss: 0.0409, Entropy Loss: -86.5787\n",
      "Current Intensity: 3.7555, Max Intensity: 3.7555\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 249/500 [02:09<01:35,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250, Reward: 17.4508\n",
      "Policy Loss: -0.1020, Value Loss: 0.0498, Entropy Loss: -86.5661\n",
      "Current Intensity: 3.8289, Max Intensity: 3.8289\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 250/500 [02:12<04:17,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 250: Average Reward = 85.6176\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 255/500 [02:14<02:45,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 255, Reward: 19.7826\n",
      "Policy Loss: -0.1714, Value Loss: 0.0288, Entropy Loss: -86.5529\n",
      "Current Intensity: 4.2124, Max Intensity: 4.2124\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 260/500 [02:16<02:19,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 260, Reward: 19.3929\n",
      "Policy Loss: -0.1716, Value Loss: 0.0329, Entropy Loss: -86.6951\n",
      "Current Intensity: 4.1368, Max Intensity: 4.1368\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 265/500 [02:19<02:33,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 265, Reward: 21.7368\n",
      "Policy Loss: -0.1485, Value Loss: 0.0541, Entropy Loss: -86.6419\n",
      "Current Intensity: 4.2293, Max Intensity: 4.2293\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 270/500 [02:21<02:07,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 270, Reward: 21.3629\n",
      "Policy Loss: -0.2820, Value Loss: 0.0434, Entropy Loss: -86.6062\n",
      "Current Intensity: 4.1676, Max Intensity: 4.1676\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 274/500 [02:22<01:27,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 275, Reward: 19.6557\n",
      "Policy Loss: -0.1980, Value Loss: 0.0461, Entropy Loss: -86.7293\n",
      "Current Intensity: 4.1937, Max Intensity: 4.1937\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 275/500 [02:25<03:56,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 275: Average Reward = 81.0835\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 280/500 [02:27<02:17,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 280, Reward: 19.6007\n",
      "Policy Loss: -0.0806, Value Loss: 0.0426, Entropy Loss: -86.6025\n",
      "Current Intensity: 2.9554, Max Intensity: 2.9554\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 285/500 [02:30<02:10,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 285, Reward: 20.2280\n",
      "Policy Loss: -0.3308, Value Loss: 0.0463, Entropy Loss: -86.7120\n",
      "Current Intensity: 2.9985, Max Intensity: 2.9985\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 290/500 [02:32<01:54,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 290, Reward: 20.9783\n",
      "Policy Loss: -0.1720, Value Loss: 0.0405, Entropy Loss: -86.7123\n",
      "Current Intensity: 2.9535, Max Intensity: 2.9535\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 295/500 [02:34<02:04,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 295, Reward: 17.6845\n",
      "Policy Loss: -0.2106, Value Loss: 0.0525, Entropy Loss: -86.5376\n",
      "Current Intensity: 2.8947, Max Intensity: 2.8947\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 299/500 [02:35<01:11,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300, Reward: 17.9329\n",
      "Policy Loss: -0.1964, Value Loss: 0.0399, Entropy Loss: -86.5023\n",
      "Current Intensity: 3.0021, Max Intensity: 3.0021\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [02:38<03:25,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 300: Average Reward = 79.0618\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 305/500 [02:40<02:06,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 305, Reward: 18.5409\n",
      "Policy Loss: -0.0363, Value Loss: 0.0506, Entropy Loss: -86.5553\n",
      "Current Intensity: 3.6215, Max Intensity: 3.6215\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 310/500 [02:43<01:49,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 310, Reward: 20.9036\n",
      "Policy Loss: -0.1506, Value Loss: 0.0378, Entropy Loss: -86.3521\n",
      "Current Intensity: 3.0287, Max Intensity: 3.0287\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 315/500 [02:45<01:54,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 315, Reward: 21.5049\n",
      "Policy Loss: -0.1154, Value Loss: 0.0562, Entropy Loss: -86.2765\n",
      "Current Intensity: 3.7652, Max Intensity: 3.7652\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 320/500 [02:48<01:58,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 320, Reward: 20.1053\n",
      "Policy Loss: -0.2511, Value Loss: 0.0377, Entropy Loss: -86.1793\n",
      "Current Intensity: 3.7162, Max Intensity: 3.7162\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 324/500 [02:49<01:15,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 325, Reward: 23.4513\n",
      "Policy Loss: -0.1268, Value Loss: 0.0547, Entropy Loss: -86.1018\n",
      "Current Intensity: 2.9616, Max Intensity: 2.9616\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 325/500 [02:52<02:59,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 325: Average Reward = 88.2245\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 330/500 [02:54<02:01,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 330, Reward: 20.3847\n",
      "Policy Loss: -0.1534, Value Loss: 0.0496, Entropy Loss: -86.1527\n",
      "Current Intensity: 4.0907, Max Intensity: 4.0907\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 335/500 [02:57<01:35,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 335, Reward: 20.9594\n",
      "Policy Loss: -0.0203, Value Loss: 0.0368, Entropy Loss: -86.1170\n",
      "Current Intensity: 4.1639, Max Intensity: 4.1639\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 340/500 [02:59<01:35,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 340, Reward: 21.5021\n",
      "Policy Loss: -0.1108, Value Loss: 0.0628, Entropy Loss: -86.0788\n",
      "Current Intensity: 4.2231, Max Intensity: 4.2231\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 345/500 [03:01<01:32,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 345, Reward: 21.2726\n",
      "Policy Loss: -0.2169, Value Loss: 0.0395, Entropy Loss: -86.0216\n",
      "Current Intensity: 4.2347, Max Intensity: 4.2347\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 349/500 [03:03<00:56,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 350, Reward: 24.3631\n",
      "Policy Loss: -0.2222, Value Loss: 0.0511, Entropy Loss: -86.0762\n",
      "Current Intensity: 4.1489, Max Intensity: 4.1489\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 350/500 [03:07<03:49,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 350: Average Reward = 92.4545\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 355/500 [03:09<01:53,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 355, Reward: 23.7425\n",
      "Policy Loss: -0.1397, Value Loss: 0.0533, Entropy Loss: -86.0160\n",
      "Current Intensity: 4.6986, Max Intensity: 4.6986\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 360/500 [03:12<01:26,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 360, Reward: 21.3717\n",
      "Policy Loss: -0.0113, Value Loss: 0.0486, Entropy Loss: -86.0521\n",
      "Current Intensity: 4.5595, Max Intensity: 4.5595\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 365/500 [03:14<01:24,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 365, Reward: 21.1826\n",
      "Policy Loss: -0.1379, Value Loss: 0.0455, Entropy Loss: -86.0514\n",
      "Current Intensity: 4.5728, Max Intensity: 4.5728\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 370/500 [03:17<01:20,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 370, Reward: 21.3997\n",
      "Policy Loss: -0.1835, Value Loss: 0.0634, Entropy Loss: -85.9051\n",
      "Current Intensity: 4.6061, Max Intensity: 4.6061\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 374/500 [03:18<00:47,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 375, Reward: 20.2349\n",
      "Policy Loss: -0.2040, Value Loss: 0.0440, Entropy Loss: -85.7569\n",
      "Current Intensity: 4.6474, Max Intensity: 4.6474\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 375/500 [03:21<02:17,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 375: Average Reward = 92.5660\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 380/500 [03:23<01:18,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 380, Reward: 20.6712\n",
      "Policy Loss: -0.1624, Value Loss: 0.0410, Entropy Loss: -85.7601\n",
      "Current Intensity: 4.6534, Max Intensity: 4.6534\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 385/500 [03:26<01:07,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 385, Reward: 22.4509\n",
      "Policy Loss: -0.1823, Value Loss: 0.0494, Entropy Loss: -85.8204\n",
      "Current Intensity: 4.6837, Max Intensity: 4.6837\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 390/500 [03:28<01:05,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 390, Reward: 20.7485\n",
      "Policy Loss: -0.1924, Value Loss: 0.0351, Entropy Loss: -85.6642\n",
      "Current Intensity: 3.7299, Max Intensity: 3.7299\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 395/500 [03:30<01:03,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 395, Reward: 23.2286\n",
      "Policy Loss: 0.0330, Value Loss: 0.0537, Entropy Loss: -85.6256\n",
      "Current Intensity: 4.7516, Max Intensity: 4.7516\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 399/500 [03:32<00:41,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400, Reward: 20.3909\n",
      "Policy Loss: -0.0917, Value Loss: 0.0523, Entropy Loss: -85.6933\n",
      "Current Intensity: 4.2272, Max Intensity: 4.2272\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [03:35<01:48,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 400: Average Reward = 99.6503\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 405/500 [03:37<01:15,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 405, Reward: 22.7137\n",
      "Policy Loss: -0.2840, Value Loss: 0.0722, Entropy Loss: -85.5734\n",
      "Current Intensity: 5.2835, Max Intensity: 5.2835\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 410/500 [03:40<01:09,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 410, Reward: 20.5536\n",
      "Policy Loss: -0.1140, Value Loss: 0.0437, Entropy Loss: -85.6281\n",
      "Current Intensity: 5.2722, Max Intensity: 5.2722\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 415/500 [03:43<01:09,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 415, Reward: 19.9573\n",
      "Policy Loss: -0.1173, Value Loss: 0.0357, Entropy Loss: -85.6969\n",
      "Current Intensity: 5.1851, Max Intensity: 5.1851\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 420/500 [03:46<00:58,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 420, Reward: 21.9525\n",
      "Policy Loss: -0.1971, Value Loss: 0.0587, Entropy Loss: -85.5119\n",
      "Current Intensity: 5.1492, Max Intensity: 5.1492\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 424/500 [03:48<00:37,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 425, Reward: 21.6329\n",
      "Policy Loss: -0.2196, Value Loss: 0.0710, Entropy Loss: -85.5231\n",
      "Current Intensity: 5.2070, Max Intensity: 5.2070\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 425/500 [03:51<01:43,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 425: Average Reward = 99.9848\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 430/500 [03:55<01:02,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 430, Reward: 24.5527\n",
      "Policy Loss: -0.1630, Value Loss: 0.0294, Entropy Loss: -85.1733\n",
      "Current Intensity: 5.2217, Max Intensity: 5.2217\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 435/500 [03:57<00:47,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 435, Reward: 19.7714\n",
      "Policy Loss: -0.2293, Value Loss: 0.0565, Entropy Loss: -85.2968\n",
      "Current Intensity: 5.1850, Max Intensity: 5.1850\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 440/500 [04:00<00:44,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 440, Reward: 20.6203\n",
      "Policy Loss: -0.1586, Value Loss: 0.0483, Entropy Loss: -85.5396\n",
      "Current Intensity: 5.2630, Max Intensity: 5.2630\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 445/500 [04:03<00:40,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 445, Reward: 23.1243\n",
      "Policy Loss: -0.0725, Value Loss: 0.0536, Entropy Loss: -85.3825\n",
      "Current Intensity: 5.1871, Max Intensity: 5.1871\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 449/500 [04:05<00:24,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 450, Reward: 24.8271\n",
      "Policy Loss: -0.1354, Value Loss: 0.0820, Entropy Loss: -85.4465\n",
      "Current Intensity: 5.2167, Max Intensity: 5.2167\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 450/500 [04:08<01:01,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 450: Average Reward = 102.6954\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 455/500 [04:11<00:38,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 455, Reward: 23.0717\n",
      "Policy Loss: 0.0506, Value Loss: 0.0632, Entropy Loss: -85.3723\n",
      "Current Intensity: 5.3338, Max Intensity: 5.3338\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 460/500 [04:14<00:31,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 460, Reward: 25.1200\n",
      "Policy Loss: -0.1543, Value Loss: 0.0605, Entropy Loss: -85.4982\n",
      "Current Intensity: 5.1754, Max Intensity: 5.1754\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 465/500 [04:17<00:25,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 465, Reward: 22.4024\n",
      "Policy Loss: -0.0989, Value Loss: 0.0444, Entropy Loss: -85.4529\n",
      "Current Intensity: 5.1969, Max Intensity: 5.1969\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 470/500 [04:20<00:24,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 470, Reward: 22.1612\n",
      "Policy Loss: -0.3041, Value Loss: 0.0518, Entropy Loss: -85.3164\n",
      "Current Intensity: 5.3112, Max Intensity: 5.3112\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 474/500 [04:22<00:12,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 475, Reward: 22.1897\n",
      "Policy Loss: -0.1130, Value Loss: 0.0432, Entropy Loss: -85.1451\n",
      "Current Intensity: 5.3003, Max Intensity: 5.3003\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 475/500 [04:25<00:32,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 475: Average Reward = 109.4737\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 480/500 [04:28<00:16,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 480, Reward: 22.1540\n",
      "Policy Loss: -0.2179, Value Loss: 0.0574, Entropy Loss: -85.1801\n",
      "Current Intensity: 5.3350, Max Intensity: 5.3350\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 485/500 [04:31<00:11,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 485, Reward: 23.0255\n",
      "Policy Loss: -0.0496, Value Loss: 0.0350, Entropy Loss: -85.1322\n",
      "Current Intensity: 5.3013, Max Intensity: 5.3013\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 490/500 [04:34<00:07,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 490, Reward: 21.5783\n",
      "Policy Loss: -0.2035, Value Loss: 0.0547, Entropy Loss: -84.9415\n",
      "Current Intensity: 5.2498, Max Intensity: 5.2498\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 495/500 [04:38<00:04,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 495, Reward: 22.1507\n",
      "Policy Loss: -0.1889, Value Loss: 0.0408, Entropy Loss: -84.8075\n",
      "Current Intensity: 4.9643, Max Intensity: 4.9643\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [04:39<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500, Reward: 22.8180\n",
      "Policy Loss: -0.3219, Value Loss: 0.0555, Entropy Loss: -84.9236\n",
      "Current Intensity: 5.1715, Max Intensity: 5.1715\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:44<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at episode 500: Average Reward = 109.7120\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Best intensity achieved: 5.5385\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class WavefrontEnv:\n",
    "    def __init__(self, slm_dim1=64, slm_dim2=64, eng_size=1,\n",
    "                 num_pix_per_block=32, alpha=0.3, noise_sigma=0.05, k=0.45, phi=None):\n",
    "        # geometry\n",
    "        self.slm_dim1, self.slm_dim2 = slm_dim1, slm_dim2\n",
    "        self.n_pix   = slm_dim1 * slm_dim2            \n",
    "        self.eng_size = eng_size                      \n",
    "\n",
    "        # phase mask\n",
    "        if phi is None:\n",
    "            self.phi = np.random.rand(slm_dim1, slm_dim2)\n",
    "        else:\n",
    "            assert phi.shape == (slm_dim1, slm_dim2), \"Phase mask shape mismatch.\"\n",
    "            self.phi = phi\n",
    "\n",
    "        # block grid\n",
    "        self.blocks = self._make_blocks(num_pix_per_block)\n",
    "        self.num_blocks = len(self.blocks)            \n",
    "\n",
    "        # RL bookkeeping\n",
    "        self.state_dim    = self.num_blocks           \n",
    "        self.action_space = self.num_blocks\n",
    "        self.alpha  = alpha\n",
    "        self.sigma  = noise_sigma                     \n",
    "        self.I0_mean = 0.0\n",
    "        self.I_max  = 0.0\n",
    "        self.I_t   = 0.0\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "        self.best_mask = None\n",
    "\n",
    "        self.set_I0_mean()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    #utils\n",
    "    def _make_blocks(self, p_per_block):\n",
    "        \"\"\"Return list of numpy arrays, each array holds pixel indices of one block.\"\"\"\n",
    "        idx = np.arange(self.n_pix)\n",
    "        return [idx[k : k + p_per_block]\n",
    "                for k in range(0, self.n_pix, p_per_block)]\n",
    "\n",
    "    def _blocks_to_pixels(self):\n",
    "        \"\"\"Convert block-level mask → 2-D pixel mask.\"\"\"\n",
    "        pixel = np.zeros(self.n_pix, dtype=np.float32)\n",
    "        for bid, bit in enumerate(self.block_mask):\n",
    "            if bit:                                     \n",
    "                pixel[self.blocks[bid]] = 1.0\n",
    "        return pixel.reshape(self.slm_dim1, self.slm_dim2)\n",
    "\n",
    "    def reset(self):\n",
    "        if self.best_mask is not None:\n",
    "            if np.random.rand() < 0.2:\n",
    "                self.block_mask = self.best_mask.copy()\n",
    "\n",
    "                # Flip a small number of bits randomly (e.g., 5 out of 128)\n",
    "                flip_indices = np.random.choice(self.num_blocks, size=5, replace=False)\n",
    "                for idx in flip_indices:\n",
    "                    self.block_mask[idx] = 1.0 - self.block_mask[idx]\n",
    "            else:\n",
    "                self.block_mask = self.best_mask.copy()\n",
    "        else:\n",
    "            self.block_mask = np.random.choice([0.0, 1.0], size=self.num_blocks).astype(np.float32)\n",
    "            \n",
    "        self.I_prev = self._intensity()\n",
    "        self.I_max  = self.I_prev\n",
    "        return self._state()\n",
    "    \n",
    "    def tanh_reward(self, I_1):\n",
    "        return np.tanh(self.k * I_1)\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        for a in action:\n",
    "            self.block_mask[a] = 1.0 - self.block_mask[a]\n",
    "\n",
    "        self.I_t = self._intensity()\n",
    "        reward = self.tanh_reward(self.I_t)\n",
    "         \n",
    "        # Bookkeeping best mask\n",
    "        if self.I_t > self.I_max:\n",
    "            self.I_max = self.I_t\n",
    "            self.best_mask = self.block_mask.copy()\n",
    "\n",
    "        return self._state(), reward\n",
    "\n",
    "    #optics\n",
    "    def _intensity(self):\n",
    "        mask2d = self._blocks_to_pixels()\n",
    "        field  = np.exp(1j * 2 * np.pi * self.phi) * mask2d\n",
    "        spec   = np.fft.fftshift(np.fft.fft2(field))\n",
    "        I      = np.abs(spec[self.slm_dim1 // 2, self.slm_dim2 // 2])**2 / spec.size\n",
    "        I     += self.sigma * np.random.randn()         # additive Gaussian noise\n",
    "        return float(I)\n",
    "\n",
    "    #state\n",
    "    def _state(self):\n",
    "        return self.block_mask.astype(np.float32)\n",
    "    \n",
    "    def set_I0_mean(self):\n",
    "        for i in range(1000):\n",
    "            self.block_mask = np.random.choice([0.0, 1.0], size=self.num_blocks).astype(np.float32)\n",
    "            I = self._intensity()\n",
    "            self.I0_mean += I\n",
    "        self.I0_mean /= 1000.0\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor_head = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "\n",
    "        # Critic head\n",
    "        self.critic_head = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        action_logits = self.actor_head(x)\n",
    "        \n",
    "        # Critic: State value\n",
    "        state_value = self.critic_head(x)\n",
    "        \n",
    "        # return action_probs, state_value\n",
    "        return action_logits, state_value\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        action_logits, _ = self.forward(state)         # [1, 128]\n",
    "    \n",
    "        dist = torch.distributions.Bernoulli(logits=action_logits)\n",
    "        \n",
    "        if deterministic:\n",
    "            # Using threshold 0.5 to binarize\n",
    "            action = (torch.sigmoid(action_logits) > 0.5).float()\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        # Convert to list of indices where action is 1\n",
    "        action = action.squeeze(0).detach().cpu().numpy().astype(int)\n",
    "        flip_indices = np.where(action == 1)[0].tolist()\n",
    "        \n",
    "        return flip_indices, action\n",
    "    \n",
    "    def evaluate(self, states, actions):\n",
    "        action_logits, state_values = self.forward(states)\n",
    "        dist = torch.distributions.Bernoulli(logits=action_logits)\n",
    "        \n",
    "        # Element-wise log-probabilities: shape [T, 128]\n",
    "        action_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        # Sum over action dimensions to get [T]\n",
    "        log_probs = action_log_probs.sum(dim=1)\n",
    "\n",
    "        # Entropy per sample (sum over 128 dimensions): shape [T]\n",
    "        dist_entropy = dist.entropy().sum(dim=1)\n",
    "\n",
    "        return log_probs, state_values, dist_entropy\n",
    "\n",
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, actor_lr=3e-4, critic_lr=3e-4, gamma=0.99, \n",
    "                 clip_ratio=0.2, value_coef=0.5, entropy_coef=0.01, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.actor_optimizer = optim.Adam(self.policy.actor_head.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.policy.critic_head.parameters(), lr=critic_lr)\n",
    "        \n",
    "        self.old_policy = copy.deepcopy(self.policy)\n",
    "        self.old_policy.eval()  # Set to evaluation mode\n",
    "    \n",
    "    def update(self, states, actions, rewards, next_states, steps_per_epoch=128, epochs=10):\n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        actions = torch.FloatTensor(np.array(actions))\n",
    "        rewards = torch.FloatTensor(np.array(rewards))\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "\n",
    "        kl_divergences = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            old_action_log_probs, old_values, _ = self.old_policy.evaluate(states, actions)\n",
    "            old_values = old_values.squeeze(-1)\n",
    "            next_values = self.old_policy.forward(next_states)[1].squeeze(-1)\n",
    "            values = torch.cat([old_values, next_values[-1].unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Calculate advantages using GAE (Generalized Advantage Estimation)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        gae = 0\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            next_value = values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * next_value - old_values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * gae\n",
    "            advantages[t] = gae\n",
    "        \n",
    "        # Compute returns (used for value function loss)\n",
    "        returns = advantages + old_values.detach()\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # PPO update loop\n",
    "        for _ in range(epochs):\n",
    "            # Create random indices for minibatches\n",
    "            indices = np.random.permutation(len(states))\n",
    "            \n",
    "            # Iterate through mini-batches\n",
    "            for start in range(0, len(states), steps_per_epoch):\n",
    "                end = start + steps_per_epoch\n",
    "                if end > len(states):\n",
    "                    end = len(states)\n",
    "                    \n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                # Get batch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_old_action_log_probs = old_action_log_probs[batch_indices]\n",
    "                \n",
    "                # Evaluate current policy\n",
    "                action_log_probs, values, entropy = self.policy.evaluate(batch_states, batch_actions)\n",
    "                \n",
    "                # Calculate ratios\n",
    "                ratios = torch.exp(action_log_probs - batch_old_action_log_probs)\n",
    "\n",
    "                kl = (batch_old_action_log_probs - action_log_probs).mean().item()  \n",
    "                kl_divergences.append(kl)\n",
    "                \n",
    "                # Compute surrogate losses\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * batch_advantages\n",
    "                \n",
    "                # Calculate loss components\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                # value_loss\n",
    "                value_loss = F.smooth_l1_loss(values.squeeze(-1), batch_returns)\n",
    "                entropy_loss = -entropy.mean()\n",
    "\n",
    "                actor_loss = policy_loss + self.entropy_coef * entropy_loss\n",
    "                critic_loss = self.value_coef * value_loss\n",
    "\n",
    "                # Update actor and critic\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy.actor_head.parameters(), 0.5)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy.critic_head.parameters(), 0.5)\n",
    "                self.critic_optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            new_values = self.policy.forward(states)[1].squeeze(-1)\n",
    "\n",
    "        # Update old policy\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        average_kl = np.mean(kl_divergences)\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item(), entropy_loss.item(), advantages, old_values, new_values, returns, average_kl\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        return self.policy.get_action(state, deterministic)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.policy.load_state_dict(torch.load(path))\n",
    "        self.old_policy.load_state_dict(torch.load(path))\n",
    "\n",
    "# Training function\n",
    "def train_ppo(env, agent, max_episodes=1000, steps_per_episode=128, \n",
    "              update_interval=10, eval_interval=50, verbose=True):\n",
    "    \n",
    "    # Initialize logging variables\n",
    "    episode_rewards = []\n",
    "    eval_rewards = []\n",
    "    best_intensity = 0\n",
    "    best_mask = None\n",
    "\n",
    "    # Storage for the all states, actions, rewards, and next_states\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "\n",
    "    all_advantage_means = []\n",
    "\n",
    "    value_losses = []\n",
    "    policy_losses = []\n",
    "    entropy_losses = []\n",
    "    all_old_values = []\n",
    "    all_new_values = []\n",
    "    all_returns = []\n",
    "    all_kl_divergences = []\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(1, max_episodes + 1)):\n",
    "        \n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Generate trajectory\n",
    "        for step in range(steps_per_episode):\n",
    "            # Select action\n",
    "            action, action_mask = agent.get_action(state)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            states.append(state)\n",
    "            actions.append(action_mask)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "        # Store episode reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Update agent\n",
    "        if episode % update_interval == 0:\n",
    "            policy_loss, value_loss, entropy_loss, advantages, old_values, new_values, returns, kl_divergence = agent.update(\n",
    "                states, actions, rewards, next_states\n",
    "            )\n",
    "            \n",
    "            advantages = advantages.detach().cpu().numpy().reshape(update_interval, steps_per_episode)\n",
    "            per_episode_advantage_means = advantages.mean(axis=1)\n",
    "\n",
    "            all_advantage_means.extend(per_episode_advantage_means)\n",
    "\n",
    "            # Append scalar value loss\n",
    "            value_losses.append(value_loss)\n",
    "            policy_losses.append(policy_loss)\n",
    "            entropy_losses.append(entropy_loss)\n",
    "\n",
    "            # Detach and move tensors to CPU\n",
    "            all_old_values.append(old_values.detach().cpu().numpy())\n",
    "            all_new_values.append(new_values.detach().cpu().numpy())\n",
    "            all_returns.append(returns.detach().cpu().numpy())\n",
    "            all_kl_divergences.append(kl_divergence)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode}, Reward: {episode_reward:.4f}\")\n",
    "                print(f\"Policy Loss: {policy_loss:.4f}, Value Loss: {value_loss:.4f}, Entropy Loss: {entropy_loss:.4f}\")\n",
    "                print(f\"Current Intensity: {env.I_prev:.4f}, Max Intensity: {env.I_max:.4f}\")\n",
    "                print(\"---\")\n",
    "            \n",
    "            states.clear()\n",
    "            actions.clear()\n",
    "            rewards.clear()\n",
    "            next_states.clear()\n",
    "        \n",
    "        # Track best solution found\n",
    "        if env.I_max > best_intensity:\n",
    "            best_intensity = env.I_max\n",
    "            best_mask = env.block_mask.copy()\n",
    "        \n",
    "        # Evaluate performance\n",
    "        if episode % eval_interval == 0:\n",
    "            eval_reward = evaluate_agent(env, agent, num_episodes=5)\n",
    "            eval_rewards.append(eval_reward)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Evaluation at episode {episode}: Average Reward = {eval_reward:.4f}\")\n",
    "                print(\"===================================\")\n",
    "    \n",
    "    old_values_all = np.concatenate(all_old_values)\n",
    "    new_values_all = np.concatenate(all_new_values)\n",
    "    returns_all = np.concatenate(all_returns)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'eval_rewards': eval_rewards,\n",
    "        'best_intensity': best_intensity,\n",
    "        'best_mask': best_mask,\n",
    "        'all_advantage_means': all_advantage_means,\n",
    "        'value_losses': value_losses,\n",
    "        'old_values_all': old_values_all,\n",
    "        'new_values_all': new_values_all,\n",
    "        'returns_all': returns_all,\n",
    "        'policy_losses': policy_losses,\n",
    "        'entropy_losses': entropy_losses,\n",
    "        'all_kl_divergences': all_kl_divergences,\n",
    "    }\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_agent(env, agent, num_episodes=5, steps_per_episode=128):\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in range(steps_per_episode):\n",
    "            action, _ = agent.get_action(state, deterministic=True)  # Use deterministic policy for evaluation\n",
    "            next_state, reward = env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "                \n",
    "        total_rewards += episode_reward\n",
    "        \n",
    "    return total_rewards / num_episodes\n",
    "\n",
    "# Visualization functions \n",
    "def plot_training_curve(results, run): \n",
    "\n",
    "    plt.figure(figsize=(16, 16))\n",
    "\n",
    "    # Plot episode rewards\n",
    "    plt.subplot(7, 1, 1)\n",
    "    plt.plot(results['episode_rewards'], label='Episode Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Rewards')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot evaluation rewards if available\n",
    "    if len(results['eval_rewards']) > 0:\n",
    "        plt.subplot(7, 1, 2)\n",
    "        eval_x = np.linspace(0, len(results['episode_rewards']), len(results['eval_rewards']))\n",
    "        plt.plot(eval_x, results['eval_rewards'], label='Evaluation Reward', color='orange')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title('Evaluation Rewards')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    # Plot mean advantage per episode\n",
    "    if 'all_advantage_means' in results and len(results['all_advantage_means']) > 0:\n",
    "        plt.subplot(7, 1, 3)\n",
    "        update_x = np.linspace(0, len(results['episode_rewards']), len(results['all_advantage_means']))\n",
    "        plt.plot(update_x, results['all_advantage_means'], label='Mean Advantage', color='green')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Mean Advantage')\n",
    "        plt.title('Advantage Trend Over Updates')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    # Plot value loss over updates\n",
    "    if 'value_losses' in results and len(results['value_losses']) > 0:\n",
    "        plt.subplot(7, 1, 4)\n",
    "        plt.plot(results['value_losses'], label='Value Loss', color='red')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Value Loss')\n",
    "        plt.title('Value Loss Trend Over Updates')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    # Plot policy loss over updates\n",
    "    if 'policy_losses' in results and len(results['policy_losses']) > 0:\n",
    "        plt.subplot(7, 1, 5)\n",
    "        plt.plot(results['policy_losses'], label='Policy Loss', color='blue')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Policy Loss')\n",
    "        plt.title('Policy Loss Trend Over Updates')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    # Plot entropy loss over updates\n",
    "    if 'entropy_losses' in results and len(results['entropy_losses']) > 0:\n",
    "        plt.subplot(7, 1, 6)\n",
    "        plt.plot(results['entropy_losses'], label='Entropy Loss', color='purple')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Entropy Loss')\n",
    "        plt.title('Entropy Loss Trend Over Updates')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    # Plot KL divergence over updates\n",
    "    if 'all_kl_divergences' in results and len(results['all_kl_divergences']) > 0:\n",
    "        plt.subplot(7, 1, 7)\n",
    "        plt.plot(results['all_kl_divergences'], label='KL Divergence', color='brown')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('KL Divergence')\n",
    "        plt.title('KL Divergence Trend Over Updates')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/run_{run}_training_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot value estimates vs returns\n",
    "    if 'old_values_all' in results and 'returns_all' in results:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(results['returns_all'], results['old_values_all'], alpha=0.5, label='Old Value Estimates')\n",
    "        plt.scatter(results['returns_all'], results['new_values_all'], alpha=0.5, label='New Value Estimates')\n",
    "        plt.plot(results['returns_all'], results['returns_all'], 'k--', label='Ideal Match')\n",
    "        plt.xlabel('Returns')\n",
    "        plt.ylabel('Value Estimates')\n",
    "        plt.title('Value Estimates vs Returns')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'results/run_{run}_value_vs_returns.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def visualize_best_mask(env, best_mask, run):\n",
    "    # Store original mask\n",
    "    original_mask = env.block_mask.copy()\n",
    "    \n",
    "    # Set the best mask\n",
    "    env.block_mask = best_mask.copy()\n",
    "    \n",
    "    # Get pixel mask\n",
    "    pixel_mask = env._blocks_to_pixels()\n",
    "    \n",
    "    # Get intensity\n",
    "    intensity = env._intensity()\n",
    "    \n",
    "    # Plot mask\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(pixel_mask, cmap='viridis')\n",
    "    plt.colorbar(label='Mask Value')\n",
    "    plt.title(f'Best Mask (Intensity: {intensity:.4f})')\n",
    "    plt.savefig(f'results/run_{run}_best_mask.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Restore original mask\n",
    "    env.block_mask = original_mask\n",
    "\n",
    "def main(run):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    phi_path = \"phase_mask/phi_64.npy\"\n",
    "\n",
    "    if os.path.exists(phi_path):\n",
    "        phi = np.load(phi_path)\n",
    "    else:\n",
    "        phi = np.random.rand(64, 64)\n",
    "        os.makedirs(os.path.dirname(phi_path), exist_ok=True)\n",
    "        np.save(phi_path, phi)\n",
    "    \n",
    "\n",
    "    # Create environment\n",
    "    env = WavefrontEnv(slm_dim1=64, slm_dim2=64, num_pix_per_block=32, alpha=0.3, noise_sigma=0.05, k=0.45, phi=phi)\n",
    "    \n",
    "    # Create PPO agent\n",
    "    agent = PPOAgent(\n",
    "        state_dim=env.state_dim,\n",
    "        action_dim=env.action_space,\n",
    "        lr=3e-4,\n",
    "        actor_lr=3e-4,\n",
    "        critic_lr=1e-4,\n",
    "        gamma=0.95,\n",
    "        clip_ratio=0.2,\n",
    "        value_coef=0.7,\n",
    "        entropy_coef=0.001,\n",
    "        gae_lambda=0.95\n",
    "    )\n",
    "    \n",
    "    # Train agent\n",
    "    print(\"Starting training...\")\n",
    "    results = train_ppo(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        max_episodes=3000,  # Adjust based on your time constraints\n",
    "        steps_per_episode=128,\n",
    "        update_interval=5,\n",
    "        eval_interval=25\n",
    "    )\n",
    "    \n",
    "    # Visualize results\n",
    "    plot_training_curve(results, run)\n",
    "    visualize_best_mask(env, results['best_mask'], run)\n",
    "    \n",
    "    print(f\"Training completed. Best intensity achieved: {results['best_intensity']:.4f}\")\n",
    "\n",
    "\n",
    "main(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
