{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43232974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Binary GA with AC integration...\n",
      "Population size: 900, Generations: 8\n",
      "SLM dimensions: 16x16, Pixels: 256\n",
      "Network stabilization: 1 gens with 1 epochs\n",
      "Normal training: 1 epochs every 1 generations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUJATHA\\AppData\\Local\\Temp\\ipykernel_11184\\1250685451.py:182: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  states = torch.FloatTensor(batch['states'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Intensive AC Update] Gen 1: Actor Loss = -88.1438, Critic Loss = 0.3720\n",
      "Gen   1  best = 2.725623\n",
      "[Normal AC Update] Gen 2: Actor Loss = -79.6213, Critic Loss = 0.3862\n",
      "Gen   2  best = 2.814578\n",
      "[Normal AC Update] Gen 3: Actor Loss = -72.6002, Critic Loss = 0.3898\n",
      "Gen   3  best = 3.779778\n",
      "[Normal AC Update] Gen 4: Actor Loss = -65.7730, Critic Loss = 0.4026\n",
      "Gen   4  best = 5.232479\n",
      "[Normal AC Update] Gen 5: Actor Loss = -60.3191, Critic Loss = 0.4155\n",
      "Gen   5  best = 5.234857\n",
      "[Normal AC Update] Gen 6: Actor Loss = -52.5538, Critic Loss = 0.4205\n",
      "Gen   6  best = 5.774911\n",
      "[Normal AC Update] Gen 7: Actor Loss = -48.7117, Critic Loss = 0.4186\n",
      "Gen   7  best = 6.570865\n",
      "[Normal AC Update] Gen 8: Actor Loss = -44.4934, Critic Loss = 0.4230\n",
      "Gen   8  best = 8.121511\n",
      "\n",
      "Final best intensity: 8.121511\n",
      "Improvement: 197.97%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def intensity(phi, mask, slm_dim1, slm_dim2, sigma):\n",
    "    # Calculate intensity at center pixel with noise\n",
    "    mask2d = mask.reshape(slm_dim1, slm_dim2)\n",
    "    field = np.exp(1j * 2 * np.pi * phi.reshape(slm_dim1, slm_dim2)) * mask2d\n",
    "    spec = np.fft.fftshift(np.fft.fft2(field))\n",
    "    I = np.abs(spec[slm_dim1 // 2, slm_dim2 // 2])**2 / spec.size\n",
    "    I += sigma * np.random.randn()  # additive Gaussian noise\n",
    "    return float(I)\n",
    "\n",
    "def random_mask(slm_dim1, slm_dim2) -> np.ndarray:\n",
    "    return np.random.choice([0.0, 1.0], size=slm_dim1*slm_dim2).astype(np.float32)\n",
    "\n",
    "class ActorCriticGA(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Actor\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, action_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.FloatTensor(state)\n",
    "\n",
    "        offspring_probs = self.actor(state)\n",
    "        state_value = self.critic(state)\n",
    "        return offspring_probs, state_value\n",
    "\n",
    "    def generate_offspring(self, state, deterministic=False):\n",
    "        offspring_probs, value = self.forward(state)\n",
    "        dist = torch.distributions.Bernoulli(offspring_probs)\n",
    "\n",
    "        if deterministic:\n",
    "            offspring_mask = (offspring_probs > 0.5).float()\n",
    "        else:\n",
    "            offspring_mask = dist.sample()\n",
    "\n",
    "        log_probs = dist.log_prob(offspring_mask)\n",
    "        return offspring_mask, log_probs, value, offspring_probs\n",
    "\n",
    "    def evaluate(self, states, actions):\n",
    "        offspring_probs, state_values = self.forward(states)\n",
    "        dist = torch.distributions.Bernoulli(offspring_probs)\n",
    "        log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "        entropy = dist.entropy().sum(dim=1)\n",
    "        return log_probs, state_values, entropy\n",
    "\n",
    "    \n",
    "\n",
    "class ACAgent:\n",
    "    \n",
    "    def __init__(self, input_dim, action_dim, actor_lr=0.001, critic_lr=0.001, batch_size=16, entropy_coef = 0.5):\n",
    "        self.policy = ActorCriticGA(input_dim, action_dim)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.policy.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.policy.critic.parameters(), lr=critic_lr)\n",
    "        self.batch_size = batch_size\n",
    "        self.entropy_coef = entropy_coef  \n",
    "        \n",
    "        self.experience_buffer = {\n",
    "            'states': [],\n",
    "            'rewards': [],\n",
    "            'log_probs': [],\n",
    "            'values': [],\n",
    "            'returns': [],\n",
    "            'advantages': [],\n",
    "            'children': []  \n",
    "        }\n",
    "        \n",
    "        self.buffer_size = 1000  # Keep last N experiences\n",
    "        self.min_buffer_size = 50  # Minimum experiences before training\n",
    "    \n",
    "    def generate_offspring(self, parent_state, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            if parent_state.dim() == 1:\n",
    "                parent_state = parent_state.unsqueeze(0)\n",
    "\n",
    "            offspring_mask, _, _, _ = self.policy.generate_offspring(parent_state, deterministic)\n",
    "            \n",
    "        return offspring_mask.squeeze(0).numpy().astype(np.float32)\n",
    "    \n",
    "\n",
    "    \n",
    "    def store_experience(self, parent_state, offspring_fitness, child):\n",
    "        # Add to experience buffer\n",
    "        self.experience_buffer['states'].append(parent_state)\n",
    "        self.experience_buffer['rewards'].append(offspring_fitness)\n",
    "        self.experience_buffer['children'].append(child)\n",
    "        \n",
    "        # Generate log_probs and value for this state-action pair\n",
    "        with torch.no_grad():\n",
    "            parent_state_tensor = torch.FloatTensor(parent_state)\n",
    "            child_tensor = torch.FloatTensor(child)\n",
    "            \n",
    "            offspring_probs, value = self.policy.forward(parent_state_tensor)\n",
    "            dist = torch.distributions.Bernoulli(offspring_probs.squeeze())\n",
    "            log_probs = dist.log_prob(child_tensor).sum()\n",
    "            \n",
    "            self.experience_buffer['log_probs'].append(log_probs.item())\n",
    "            self.experience_buffer['values'].append(value.item())\n",
    "        \n",
    "        # Keep buffer size manageable\n",
    "        if len(self.experience_buffer['states']) > self.buffer_size:\n",
    "            for key in self.experience_buffer:\n",
    "                self.experience_buffer[key] = self.experience_buffer[key][-self.buffer_size:]\n",
    "    \n",
    "    def compute_returns_and_advantages(self):\n",
    "        # Compute returns and advantages for all experiences in buffer\n",
    "        rewards = np.array(self.experience_buffer['rewards'])\n",
    "        values = np.array(self.experience_buffer['values'])\n",
    "        \n",
    "        # Compute returns (copy rewards)\n",
    "        returns = rewards.copy()\n",
    "        \n",
    "        # Normalize returns\n",
    "        if returns.std() > 1e-8:\n",
    "            returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = returns - values\n",
    "        if advantages.std() > 1e-8:\n",
    "            advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "        self.experience_buffer['returns'] = returns.tolist()\n",
    "        self.experience_buffer['advantages'] = advantages.tolist()\n",
    "    \n",
    "    def get_training_batches(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        n_samples = len(self.experience_buffer['states'])\n",
    "        indices = np.random.permutation(n_samples)\n",
    "\n",
    "        batches = []\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch = {key: [self.experience_buffer[key][i] for i in batch_indices]\n",
    "                    for key in self.experience_buffer}\n",
    "            batches.append(batch)\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def update_policy(self, n_epochs=10):\n",
    "        if len(self.experience_buffer['states']) < self.min_buffer_size:\n",
    "            return None, None\n",
    "\n",
    "        # Compute returns and advantages for all experiences\n",
    "        self.compute_returns_and_advantages()\n",
    "\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        total_entropy_loss = 0\n",
    "        total_policy_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in self.get_training_batches():\n",
    "                states = torch.FloatTensor(batch['states'])\n",
    "                returns = torch.FloatTensor(batch['returns'])\n",
    "                advantages = torch.FloatTensor(batch['advantages'])\n",
    "                children = torch.FloatTensor(batch['children'])\n",
    "\n",
    "                log_probs, state_values, entropy = self.policy.evaluate(states, children)\n",
    "\n",
    "                # Actor loss\n",
    "                policy_loss = -(log_probs * advantages).mean()\n",
    "                entropy_loss = -entropy.mean()  # Encourage exploration\n",
    "                actor_loss = policy_loss + self.entropy_coef * entropy_loss\n",
    "\n",
    "                # Critic loss\n",
    "                critic_loss = F.smooth_l1_loss(state_values.squeeze(-1), returns)\n",
    "\n",
    "                # Update networks\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.actor.parameters(), 0.5)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.critic.parameters(), 0.5)\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "                total_actor_loss += actor_loss.item()\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                total_entropy_loss += entropy_loss.item()\n",
    "                total_policy_loss += policy_loss.item()\n",
    "                n_batches += 1\n",
    "\n",
    "        avg_actor_loss = total_actor_loss / n_batches\n",
    "        avg_critic_loss = total_critic_loss / n_batches\n",
    "        avg_entropy_loss = total_entropy_loss / n_batches\n",
    "        avg_policy_loss = total_policy_loss / n_batches\n",
    "\n",
    "        self.entropy_coef = max(0.1, self.entropy_coef * 0.9)\n",
    "\n",
    "        return avg_actor_loss, avg_critic_loss, avg_entropy_loss, avg_policy_loss\n",
    "    \n",
    "    def update_policy_intensive(self, n_epochs=10):\n",
    "        # Intensive training for early generations to stabilize networks\n",
    "        return self.update_policy(n_epochs=n_epochs)\n",
    "\n",
    "\n",
    "def binary_ga(slm_dim1: int = 16,\n",
    "              slm_dim2: int = 16,\n",
    "              pop_size: int = 30,\n",
    "              generations: int = 200,\n",
    "              sigma: float = 0.0,\n",
    "              phi: np.ndarray | None = None,\n",
    "              seed: int | None = None,\n",
    "              update_freq: int = 5,\n",
    "              stabilization_gens: int = 20,\n",
    "              intensive_epochs: int = 100,\n",
    "              normal_epochs: int = 10):\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    n_pix = slm_dim1 * slm_dim2\n",
    "    if phi is None:\n",
    "        phi = np.random.rand(n_pix).astype(np.float32)\n",
    "\n",
    "    # Initialize parent population\n",
    "    population = [random_mask(slm_dim1, slm_dim2) for _ in range(pop_size)]\n",
    "    best_int_history = []\n",
    "\n",
    "    # Initialize AC agent\n",
    "    agent = ACAgent(\n",
    "        input_dim=n_pix * 2 + 2, \n",
    "        action_dim=n_pix, \n",
    "        actor_lr=0.00009, \n",
    "        critic_lr=0.00009, \n",
    "        batch_size=16, \n",
    "        entropy_coef=0.5\n",
    "    )\n",
    "\n",
    "    print(f\"Starting Binary GA with AC integration...\")\n",
    "    print(f\"Population size: {pop_size}, Generations: {generations}\")\n",
    "    print(f\"SLM dimensions: {slm_dim1}x{slm_dim2}, Pixels: {n_pix}\")\n",
    "    print(f\"Network stabilization: {stabilization_gens} gens with {intensive_epochs} epochs\")\n",
    "    print(f\"Normal training: {normal_epochs} epochs every {update_freq} generations\")\n",
    "\n",
    "    policy_losses = []\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "    entropy_losses = []\n",
    "    eval_mean_intensities = []\n",
    "    eval_max_intensities = []\n",
    "\n",
    "    for g in range(1, generations + 1):\n",
    "        # 1) Evaluate current population\n",
    "        intensities = [intensity(phi, mask, slm_dim1, slm_dim2, sigma) for mask in population]\n",
    "        order = np.argsort(intensities)[::-1]  # Descending order\n",
    "\n",
    "        # 2) Generate offspring using AC policy\n",
    "        offspring = []\n",
    "        generation_experiences = []\n",
    "        \n",
    "        for _ in range(pop_size):\n",
    "            # Select two parents from top half\n",
    "            top_half_size = pop_size // 2\n",
    "            idx_pool = np.random.choice(top_half_size, size=2, replace=False)\n",
    "            \n",
    "            parent1_idx = order[idx_pool[0]]\n",
    "            parent2_idx = order[idx_pool[1]]\n",
    "            \n",
    "            parent1 = population[parent1_idx]\n",
    "            parent2 = population[parent2_idx]\n",
    "            score1 = intensities[parent1_idx]\n",
    "            score2 = intensities[parent2_idx]\n",
    "            \n",
    "            # Generate offspring using AC policy\n",
    "            parent_state = np.concatenate([parent1, parent2, [score1], [score2]])\n",
    "            parent_state_tensor = torch.FloatTensor(parent_state)\n",
    "            child = agent.generate_offspring(parent_state_tensor, deterministic=False)\n",
    "            offspring.append(child)\n",
    "            \n",
    "            # Store parent info for later training\n",
    "            generation_experiences.append((parent_state, child))\n",
    "\n",
    "        # 3) Evaluate offspring\n",
    "        offspring_intensities = [intensity(phi, mask, slm_dim1, slm_dim2, sigma) for mask in offspring]\n",
    "\n",
    "        # 4) Store experiences for AC training\n",
    "        for i, (parent_state, child) in enumerate(generation_experiences):\n",
    "            offspring_fitness = offspring_intensities[i]\n",
    "            agent.store_experience(parent_state, offspring_fitness, child)\n",
    "\n",
    "        # 5) Replace worst half of population with best offspring\n",
    "        offspring_order = np.argsort(offspring_intensities)[::-1]\n",
    "        worst_half_size = pop_size // 2\n",
    "        for k in range(worst_half_size):\n",
    "            worst_parent_idx = order[-(k + 1)]  # Start from worst\n",
    "            best_offspring_idx = offspring_order[k]  # Best offspring\n",
    "            population[worst_parent_idx] = offspring[best_offspring_idx]\n",
    "\n",
    "        # 6) Update AC policy with appropriate training intensity\n",
    "        if g % update_freq == 0:\n",
    "            # Intensive training for early generations to stabilize networks\n",
    "            if g <= stabilization_gens:\n",
    "                actor_loss, critic_loss, entropy_loss, policy_loss = agent.update_policy_intensive(intensive_epochs)\n",
    "                training_type = \"Intensive\"\n",
    "            else:\n",
    "                actor_loss, critic_loss, entropy_loss, policy_loss = agent.update_policy(normal_epochs)\n",
    "                training_type = \"Normal\"\n",
    "            \n",
    "            eval_intensities, eval_mean_intensity = evaluate_agent_deterministic(agent, population, phi, slm_dim1, slm_dim2, sigma=0.0)\n",
    "            eval_max_intensities.append(max(eval_intensities))\n",
    "            eval_mean_intensities.append(eval_mean_intensity)\n",
    "\n",
    "\n",
    "            if actor_loss is not None and critic_loss is not None:\n",
    "                print(f\"[{training_type} AC Update] Gen {g}: Actor Loss = {actor_loss:.4f}, Critic Loss = {critic_loss:.4f}\")\n",
    "                policy_losses.append(policy_loss)\n",
    "                actor_losses.append(actor_loss)\n",
    "                critic_losses.append(critic_loss)\n",
    "                entropy_losses.append(entropy_loss)\n",
    "            else:\n",
    "                print(f\"[AC Update] Gen {g}: Insufficient data for training\")\n",
    "\n",
    "        # 7) Track best fitness\n",
    "        current_intensities = [intensity(phi, mask, slm_dim1, slm_dim2, sigma) for mask in population]\n",
    "        best_current = max(current_intensities)\n",
    "        best_int_history.append(best_current)\n",
    "\n",
    "       \n",
    "        print(f\"Gen {g:3d}  best = {best_current:.6f}\")\n",
    "\n",
    "    return {'max_intensity': best_int_history,\n",
    "            'policy_losses': policy_losses,\n",
    "            'actor_losses': actor_losses,\n",
    "            'critic_losses': critic_losses,\n",
    "            'entropy_losses': entropy_losses,\n",
    "            'eval_mean_intensities': eval_mean_intensities,\n",
    "            'eval_max_intensities': eval_max_intensities}\n",
    "\n",
    "def evaluate_agent_deterministic(agent, population, phi, slm_dim1, slm_dim2, sigma):\n",
    "    # Evaluate current policy network deterministically on given population.\n",
    "    pop_size = len(population)\n",
    "\n",
    "    # 1. Evaluate fitness of current parents\n",
    "    parent_intensities = [intensity(phi, mask, slm_dim1, slm_dim2, sigma) for mask in population]\n",
    "    order = np.argsort(parent_intensities)[::-1]\n",
    "\n",
    "    deterministic_offspring = []\n",
    "    for _ in range(pop_size):\n",
    "        # Choose parents from top half\n",
    "        top_half = pop_size // 2\n",
    "        p1_idx, p2_idx = np.random.choice(top_half, size=2, replace=False)\n",
    "        idx1, idx2 = order[p1_idx], order[p2_idx]\n",
    "\n",
    "        parent1 = population[idx1]\n",
    "        parent2 = population[idx2]\n",
    "        score1 = parent_intensities[idx1]\n",
    "        score2 = parent_intensities[idx2]\n",
    "\n",
    "        # Construct input state\n",
    "        state = np.concatenate([parent1, parent2, [score1], [score2]])\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "\n",
    "        # Generate deterministic offspring\n",
    "        child = agent.generate_offspring(state_tensor, deterministic=True)\n",
    "        deterministic_offspring.append(child)\n",
    "\n",
    "    # 2. Evaluate intensities\n",
    "    eval_intensities = [intensity(phi, child, slm_dim1, slm_dim2, sigma) for child in deterministic_offspring]\n",
    "    mean_eval_intensity = np.mean(eval_intensities)\n",
    "\n",
    "    return eval_intensities, mean_eval_intensity\n",
    "\n",
    "def plot_ga_ac_training(results, run_id='default'):\n",
    "    plt.figure(figsize=(16, 16))\n",
    "\n",
    "    # 1. Best Intensity Curve\n",
    "    if 'max_intensity' in results and len(results['max_intensity']) > 0:\n",
    "        plt.subplot(7, 1, 1)\n",
    "        plt.plot(results['max_intensity'], label='Best Intensity during Training', color='black')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Intensity')\n",
    "        plt.title('Best Intensity Per Generation')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    # 2. Actor Loss Trend\n",
    "    if 'actor_losses' in results and len(results['actor_losses']) > 0:\n",
    "        plt.subplot(7, 1, 2)\n",
    "        plt.plot(results['actor_losses'], label='Actor Loss', color='blue')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Actor Loss Over Updates')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    # 3. Critic Loss Trend\n",
    "    if 'critic_losses' in results and len(results['critic_losses']) > 0:\n",
    "        plt.subplot(7, 1, 3)\n",
    "        plt.plot(results['critic_losses'], label='Critic Loss', color='red')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Critic Loss Over Updates')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    # 4. Policy Gradient Loss (before entropy term)\n",
    "    if 'policy_losses' in results and len(results['policy_losses']) > 0:\n",
    "        plt.subplot(7, 1, 4)\n",
    "        plt.plot(results['policy_losses'], label='Policy Gradient Loss', color='green')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Policy Gradient Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "    # 5. Entropy Loss\n",
    "    if 'entropy_losses' in results and len(results['entropy_losses']) > 0:\n",
    "        plt.subplot(7, 1, 5)\n",
    "        plt.plot(results['entropy_losses'], label='Entropy Loss', color='purple')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Entropy')\n",
    "        plt.title('Entropy Loss Over Updates')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    # 6. Evaluation Mean Intensities\n",
    "    if 'eval_mean_intensities' in results and len(results['eval_mean_intensities']) > 0:\n",
    "        plt.subplot(7, 1, 6)\n",
    "        plt.plot(results['eval_mean_intensities'], label='Mean Intensity (Eval)', color='orange')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Mean Intensity')\n",
    "        plt.title('Mean Intensity of Offspring (Evaluation)')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    # 7. Evaluation Max Intensities\n",
    "    if 'eval_max_intensities' in results and len(results['eval_max_intensities']) > 0:\n",
    "        plt.subplot(7, 1, 7)\n",
    "        plt.plot(results['eval_max_intensities'], label='Max Intensity (Eval)', color='cyan')\n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Max Intensity')\n",
    "        plt.title('Max Intensity of Offspring (Evaluation)')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results_ga_rl/run_{run_id}_ga_ac_training_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main(run=0):\n",
    "\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    phi_path = \"phase_mask/phi_16.npy\"\n",
    "\n",
    "    if os.path.exists(phi_path):\n",
    "        phi = np.load(phi_path)\n",
    "    else:\n",
    "        phi = np.random.rand(16, 16).astype(np.float32)\n",
    "        os.makedirs(os.path.dirname(phi_path), exist_ok=True)\n",
    "        np.save(phi_path, phi)\n",
    "\n",
    "\n",
    "    result = binary_ga(\n",
    "        slm_dim1=16, \n",
    "        slm_dim2=16, \n",
    "        pop_size=900, \n",
    "        generations=8,\n",
    "        sigma=0.01,\n",
    "        phi=phi,\n",
    "        seed=42,\n",
    "        update_freq=1,              \n",
    "        stabilization_gens=1,      \n",
    "        intensive_epochs=1,        \n",
    "        normal_epochs=1,           \n",
    "    )\n",
    "\n",
    "    # Plot the training results\n",
    "    plot_ga_ac_training(result, run_id=run)\n",
    "\n",
    "    history = result['max_intensity']\n",
    "    print(f\"\\nFinal best intensity: {max(history):.6f}\")\n",
    "    print(f\"Improvement: {(max(history) - history[0])/history[0]*100:.2f}%\")\n",
    "\n",
    "\n",
    "main(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43367f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Binary-GA] Gen   1  Best Intensity = 1.6213\n",
      "[Binary-GA] Gen   2  Best Intensity = 2.6183\n",
      "[Binary-GA] Gen   3  Best Intensity = 2.6242\n",
      "[Binary-GA] Gen   4  Best Intensity = 2.6486\n",
      "[Binary-GA] Gen   5  Best Intensity = 3.1527\n",
      "[Binary-GA] Gen   6  Best Intensity = 3.1477\n",
      "[Binary-GA] Gen   7  Best Intensity = 4.4219\n",
      "[Binary-GA] Gen   8  Best Intensity = 5.3945\n",
      "[Binary-GA] Gen   9  Best Intensity = 5.3906\n",
      "[Binary-GA] Gen  10  Best Intensity = 5.3928\n",
      "[Binary-GA] Gen  11  Best Intensity = 5.4149\n",
      "[Binary-GA] Gen  12  Best Intensity = 5.6456\n",
      "[Binary-GA] Gen  13  Best Intensity = 5.7987\n",
      "[Binary-GA] Gen  14  Best Intensity = 6.0462\n",
      "[Binary-GA] Gen  15  Best Intensity = 6.9040\n",
      "[Binary-GA] Gen  16  Best Intensity = 6.9116\n",
      "[Binary-GA] Gen  17  Best Intensity = 6.9019\n",
      "[Binary-GA] Gen  18  Best Intensity = 6.9183\n",
      "[Binary-GA] Gen  19  Best Intensity = 7.4282\n",
      "[Binary-GA] Gen  20  Best Intensity = 8.7649\n",
      "[Binary-GA] Gen  21  Best Intensity = 8.7602\n",
      "[Binary-GA] Gen  22  Best Intensity = 8.7675\n",
      "[Binary-GA] Gen  23  Best Intensity = 8.7752\n",
      "[Binary-GA] Gen  24  Best Intensity = 9.1554\n",
      "[Binary-GA] Gen  25  Best Intensity = 9.6825\n",
      "[Binary-GA] Gen  26  Best Intensity = 9.6896\n",
      "[Binary-GA] Gen  27  Best Intensity = 9.7005\n",
      "[Binary-GA] Gen  28  Best Intensity = 10.4158\n",
      "[Binary-GA] Gen  29  Best Intensity = 10.4294\n",
      "[Binary-GA] Gen  30  Best Intensity = 10.4088\n",
      "[Binary-GA] Gen  31  Best Intensity = 10.8288\n",
      "[Binary-GA] Gen  32  Best Intensity = 11.5414\n",
      "[Binary-GA] Gen  33  Best Intensity = 12.9503\n",
      "[Binary-GA] Gen  34  Best Intensity = 13.4479\n",
      "[Binary-GA] Gen  35  Best Intensity = 13.4731\n",
      "[Binary-GA] Gen  36  Best Intensity = 13.4455\n",
      "[Binary-GA] Gen  37  Best Intensity = 13.4588\n",
      "[Binary-GA] Gen  38  Best Intensity = 13.4361\n",
      "[Binary-GA] Gen  39  Best Intensity = 13.5626\n",
      "[Binary-GA] Gen  40  Best Intensity = 13.5693\n",
      "[Binary-GA] Gen  41  Best Intensity = 13.5557\n",
      "[Binary-GA] Gen  42  Best Intensity = 13.5799\n",
      "[Binary-GA] Gen  43  Best Intensity = 13.5591\n",
      "[Binary-GA] Gen  44  Best Intensity = 13.7024\n",
      "[Binary-GA] Gen  45  Best Intensity = 16.3871\n",
      "[Binary-GA] Gen  46  Best Intensity = 16.4029\n",
      "[Binary-GA] Gen  47  Best Intensity = 16.3938\n",
      "[Binary-GA] Gen  48  Best Intensity = 16.3914\n",
      "[Binary-GA] Gen  49  Best Intensity = 16.4025\n",
      "[Binary-GA] Gen  50  Best Intensity = 16.6620\n",
      "[Binary-GA] Gen  51  Best Intensity = 16.6392\n",
      "[Binary-GA] Gen  52  Best Intensity = 16.6486\n",
      "[Binary-GA] Gen  53  Best Intensity = 16.6436\n",
      "[Binary-GA] Gen  54  Best Intensity = 17.0962\n",
      "[Binary-GA] Gen  55  Best Intensity = 17.0951\n",
      "[Binary-GA] Gen  56  Best Intensity = 17.1979\n",
      "[Binary-GA] Gen  57  Best Intensity = 17.1928\n",
      "[Binary-GA] Gen  58  Best Intensity = 17.1950\n",
      "[Binary-GA] Gen  59  Best Intensity = 17.1768\n",
      "[Binary-GA] Gen  60  Best Intensity = 17.9555\n",
      "[Binary-GA] Gen  61  Best Intensity = 17.9457\n",
      "[Binary-GA] Gen  62  Best Intensity = 18.0521\n",
      "[Binary-GA] Gen  63  Best Intensity = 18.0593\n",
      "[Binary-GA] Gen  64  Best Intensity = 18.2481\n",
      "[Binary-GA] Gen  65  Best Intensity = 18.9393\n",
      "[Binary-GA] Gen  66  Best Intensity = 19.1500\n",
      "[Binary-GA] Gen  67  Best Intensity = 19.1503\n",
      "[Binary-GA] Gen  68  Best Intensity = 20.0052\n",
      "[Binary-GA] Gen  69  Best Intensity = 19.9936\n",
      "[Binary-GA] Gen  70  Best Intensity = 20.0097\n",
      "[Binary-GA] Gen  71  Best Intensity = 20.0150\n",
      "[Binary-GA] Gen  72  Best Intensity = 20.7640\n",
      "[Binary-GA] Gen  73  Best Intensity = 20.7845\n",
      "[Binary-GA] Gen  74  Best Intensity = 21.2546\n",
      "[Binary-GA] Gen  75  Best Intensity = 21.2377\n",
      "[Binary-GA] Gen  76  Best Intensity = 21.2349\n",
      "[Binary-GA] Gen  77  Best Intensity = 21.5239\n",
      "[Binary-GA] Gen  78  Best Intensity = 22.7947\n",
      "[Binary-GA] Gen  79  Best Intensity = 22.8108\n",
      "[Binary-GA] Gen  80  Best Intensity = 22.7983\n",
      "[Binary-GA] Gen  81  Best Intensity = 22.8281\n",
      "[Binary-GA] Gen  82  Best Intensity = 22.7782\n",
      "[Binary-GA] Gen  83  Best Intensity = 22.8131\n",
      "[Binary-GA] Gen  84  Best Intensity = 22.7915\n",
      "[Binary-GA] Gen  85  Best Intensity = 22.8063\n",
      "[Binary-GA] Gen  86  Best Intensity = 22.8071\n",
      "[Binary-GA] Gen  87  Best Intensity = 23.3924\n",
      "[Binary-GA] Gen  88  Best Intensity = 23.4067\n",
      "[Binary-GA] Gen  89  Best Intensity = 23.4252\n",
      "[Binary-GA] Gen  90  Best Intensity = 24.7156\n",
      "[Binary-GA] Gen  91  Best Intensity = 24.7248\n",
      "[Binary-GA] Gen  92  Best Intensity = 24.7326\n",
      "[Binary-GA] Gen  93  Best Intensity = 24.7348\n",
      "[Binary-GA] Gen  94  Best Intensity = 24.7354\n",
      "[Binary-GA] Gen  95  Best Intensity = 24.7435\n",
      "[Binary-GA] Gen  96  Best Intensity = 24.7276\n",
      "[Binary-GA] Gen  97  Best Intensity = 25.0739\n",
      "[Binary-GA] Gen  98  Best Intensity = 25.0392\n",
      "[Binary-GA] Gen  99  Best Intensity = 25.0615\n",
      "[Binary-GA] Gen 100  Best Intensity = 25.0584\n",
      "[Binary-GA] Gen 101  Best Intensity = 26.1260\n",
      "[Binary-GA] Gen 102  Best Intensity = 26.1263\n",
      "[Binary-GA] Gen 103  Best Intensity = 26.1166\n",
      "[Binary-GA] Gen 104  Best Intensity = 26.1217\n",
      "[Binary-GA] Gen 105  Best Intensity = 26.1370\n",
      "[Binary-GA] Gen 106  Best Intensity = 26.1331\n",
      "[Binary-GA] Gen 107  Best Intensity = 26.1231\n",
      "[Binary-GA] Gen 108  Best Intensity = 26.1201\n",
      "[Binary-GA] Gen 109  Best Intensity = 26.2288\n",
      "[Binary-GA] Gen 110  Best Intensity = 27.3057\n",
      "[Binary-GA] Gen 111  Best Intensity = 27.2983\n",
      "[Binary-GA] Gen 112  Best Intensity = 27.3084\n",
      "[Binary-GA] Gen 113  Best Intensity = 27.3138\n",
      "[Binary-GA] Gen 114  Best Intensity = 27.4089\n",
      "[Binary-GA] Gen 115  Best Intensity = 27.4145\n",
      "[Binary-GA] Gen 116  Best Intensity = 27.4144\n",
      "[Binary-GA] Gen 117  Best Intensity = 27.7740\n",
      "[Binary-GA] Gen 118  Best Intensity = 27.9548\n",
      "[Binary-GA] Gen 119  Best Intensity = 27.9721\n",
      "[Binary-GA] Gen 120  Best Intensity = 27.9769\n",
      "[Binary-GA] Gen 121  Best Intensity = 27.9627\n",
      "[Binary-GA] Gen 122  Best Intensity = 27.9891\n",
      "[Binary-GA] Gen 123  Best Intensity = 28.0182\n",
      "[Binary-GA] Gen 124  Best Intensity = 28.0141\n",
      "[Binary-GA] Gen 125  Best Intensity = 28.0228\n",
      "[Binary-GA] Gen 126  Best Intensity = 28.3853\n",
      "[Binary-GA] Gen 127  Best Intensity = 28.4005\n",
      "[Binary-GA] Gen 128  Best Intensity = 28.4117\n",
      "[Binary-GA] Gen 129  Best Intensity = 28.4011\n",
      "[Binary-GA] Gen 130  Best Intensity = 28.3947\n",
      "[Binary-GA] Gen 131  Best Intensity = 28.3990\n",
      "[Binary-GA] Gen 132  Best Intensity = 28.7001\n",
      "[Binary-GA] Gen 133  Best Intensity = 28.9156\n",
      "[Binary-GA] Gen 134  Best Intensity = 28.8852\n",
      "[Binary-GA] Gen 135  Best Intensity = 29.0892\n",
      "[Binary-GA] Gen 136  Best Intensity = 29.1355\n",
      "[Binary-GA] Gen 137  Best Intensity = 29.1323\n",
      "[Binary-GA] Gen 138  Best Intensity = 29.1656\n",
      "[Binary-GA] Gen 139  Best Intensity = 29.1771\n",
      "[Binary-GA] Gen 140  Best Intensity = 29.1722\n",
      "[Binary-GA] Gen 141  Best Intensity = 29.4567\n",
      "[Binary-GA] Gen 142  Best Intensity = 29.4366\n",
      "[Binary-GA] Gen 143  Best Intensity = 29.6964\n",
      "[Binary-GA] Gen 144  Best Intensity = 29.6978\n",
      "[Binary-GA] Gen 145  Best Intensity = 29.6915\n",
      "[Binary-GA] Gen 146  Best Intensity = 30.0779\n",
      "[Binary-GA] Gen 147  Best Intensity = 30.4818\n",
      "[Binary-GA] Gen 148  Best Intensity = 30.4849\n",
      "[Binary-GA] Gen 149  Best Intensity = 30.5039\n",
      "[Binary-GA] Gen 150  Best Intensity = 30.5046\n",
      "[Binary-GA] Gen 151  Best Intensity = 30.5000\n",
      "[Binary-GA] Gen 152  Best Intensity = 30.4823\n",
      "[Binary-GA] Gen 153  Best Intensity = 30.4877\n",
      "[Binary-GA] Gen 154  Best Intensity = 30.4892\n",
      "[Binary-GA] Gen 155  Best Intensity = 30.5001\n",
      "[Binary-GA] Gen 156  Best Intensity = 30.5142\n",
      "[Binary-GA] Gen 157  Best Intensity = 30.4813\n",
      "[Binary-GA] Gen 158  Best Intensity = 30.4924\n",
      "[Binary-GA] Gen 159  Best Intensity = 30.4853\n",
      "[Binary-GA] Gen 160  Best Intensity = 30.5548\n",
      "[Binary-GA] Gen 161  Best Intensity = 30.5583\n",
      "[Binary-GA] Gen 162  Best Intensity = 30.5751\n",
      "[Binary-GA] Gen 163  Best Intensity = 30.5551\n",
      "[Binary-GA] Gen 164  Best Intensity = 30.5613\n",
      "[Binary-GA] Gen 165  Best Intensity = 30.5484\n",
      "[Binary-GA] Gen 166  Best Intensity = 30.5418\n",
      "[Binary-GA] Gen 167  Best Intensity = 30.5369\n",
      "[Binary-GA] Gen 168  Best Intensity = 30.5538\n",
      "[Binary-GA] Gen 169  Best Intensity = 30.5567\n",
      "[Binary-GA] Gen 170  Best Intensity = 30.5702\n",
      "[Binary-GA] Gen 171  Best Intensity = 30.8651\n",
      "[Binary-GA] Gen 172  Best Intensity = 30.8488\n",
      "[Binary-GA] Gen 173  Best Intensity = 30.8851\n",
      "[Binary-GA] Gen 174  Best Intensity = 30.8578\n",
      "[Binary-GA] Gen 175  Best Intensity = 30.9050\n",
      "[Binary-GA] Gen 176  Best Intensity = 30.8716\n",
      "[Binary-GA] Gen 177  Best Intensity = 30.8934\n",
      "[Binary-GA] Gen 178  Best Intensity = 31.0155\n",
      "[Binary-GA] Gen 179  Best Intensity = 31.0030\n",
      "[Binary-GA] Gen 180  Best Intensity = 31.0149\n",
      "[Binary-GA] Gen 181  Best Intensity = 31.0229\n",
      "[Binary-GA] Gen 182  Best Intensity = 31.0149\n",
      "[Binary-GA] Gen 183  Best Intensity = 31.0794\n",
      "[Binary-GA] Gen 184  Best Intensity = 31.0663\n",
      "[Binary-GA] Gen 185  Best Intensity = 31.0372\n",
      "[Binary-GA] Gen 186  Best Intensity = 31.0930\n",
      "[Binary-GA] Gen 187  Best Intensity = 31.0778\n",
      "[Binary-GA] Gen 188  Best Intensity = 31.0877\n",
      "[Binary-GA] Gen 189  Best Intensity = 31.0908\n",
      "[Binary-GA] Gen 190  Best Intensity = 31.1051\n",
      "[Binary-GA] Gen 191  Best Intensity = 31.1074\n",
      "[Binary-GA] Gen 192  Best Intensity = 31.0791\n",
      "[Binary-GA] Gen 193  Best Intensity = 31.0928\n",
      "[Binary-GA] Gen 194  Best Intensity = 31.1018\n",
      "[Binary-GA] Gen 195  Best Intensity = 31.1218\n",
      "[Binary-GA] Gen 196  Best Intensity = 31.1265\n",
      "[Binary-GA] Gen 197  Best Intensity = 31.1324\n",
      "[Binary-GA] Gen 198  Best Intensity = 31.1237\n",
      "[Binary-GA] Gen 199  Best Intensity = 31.1162\n",
      "[Binary-GA] Gen 200  Best Intensity = 31.1245\n",
      "Final best intensity: 31.132357\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def intensity(phi, mask, slm_dim1, slm_dim2, sigma):\n",
    "    mask2d = mask.reshape(slm_dim1, slm_dim2)\n",
    "    field = np.exp(1j * 2 * np.pi * phi.reshape(slm_dim1, slm_dim2)) * mask2d\n",
    "    spec = np.fft.fftshift(np.fft.fft2(field))\n",
    "    I = np.abs(spec[slm_dim1 // 2, slm_dim2 // 2])**2 / spec.size\n",
    "    I += sigma * np.random.randn()  # additive Gaussian noise\n",
    "    return float(I)\n",
    "\n",
    "def random_mask(slm_dim1, slm_dim2) -> np.ndarray:\n",
    "    return np.random.choice([0.0, 1.0], size=slm_dim1*slm_dim2).astype(np.float32)\n",
    "\n",
    "def binary_ga(slm_dim1: int = 16,\n",
    "              slm_dim2: int = 16,\n",
    "              pop_size: int = 30,\n",
    "              generations: int = 200,\n",
    "              sigma: float = 0.0,\n",
    "              R0: float = 0.1,\n",
    "              Rend: float = 0.0025,\n",
    "              lambda_: float = 50.0,\n",
    "              phi: np.ndarray | None = None,\n",
    "              seed: int | None = None):\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    n_pix = slm_dim1 * slm_dim2\n",
    "    if phi is None:\n",
    "        phi = np.random.rand(n_pix).astype(np.float32)\n",
    "\n",
    "    population = [random_mask(slm_dim1, slm_dim2) for _ in range(pop_size)]\n",
    "    best_int_history = []\n",
    "    best_mask = None\n",
    "\n",
    "    for g in range(1, generations + 1):\n",
    "        # 1) Evaluate current parents\n",
    "        intensities = [intensity(phi, m, slm_dim1, slm_dim2, sigma) for m in population]\n",
    "        order = np.argsort(intensities)[::-1]  # Descending\n",
    "\n",
    "        # 2) Crossover template\n",
    "        T_flat = np.zeros(n_pix, dtype=np.float32)\n",
    "        T_flat[np.random.choice(n_pix, n_pix // 2, replace=False)] = 1.0\n",
    "        T_breed = T_flat.reshape(slm_dim1, slm_dim2)\n",
    "\n",
    "        # 3) Generate offspring via crossover\n",
    "        offspring = []\n",
    "        for _ in range(pop_size):\n",
    "            idx_pool = np.random.permutation(pop_size // 2)[:2]\n",
    "            pa = population[order[idx_pool[0]]].reshape(slm_dim1, slm_dim2)\n",
    "            ma = population[order[idx_pool[1]]].reshape(slm_dim1, slm_dim2)\n",
    "            child = pa * T_breed + ma * (1 - T_breed)\n",
    "            offspring.append(child)\n",
    "\n",
    "        # 4) Mutate offspring with decaying rate\n",
    "        mut_rate = (R0 - Rend) * math.exp(-g / lambda_) + Rend\n",
    "        mut_num = int(round(mut_rate * n_pix))\n",
    "        mutated = []\n",
    "        for child in offspring:\n",
    "            flat = child.flatten()\n",
    "            flip_idx = np.random.choice(n_pix, mut_num, replace=False)\n",
    "            flat[flip_idx] = 1.0 - flat[flip_idx]  # Bit‑flip mutation\n",
    "            mutated.append(flat.reshape(slm_dim1, slm_dim2))\n",
    "\n",
    "        # 5) Evaluate offspring\n",
    "        off_int = [intensity(phi, m, slm_dim1, slm_dim2, sigma) for m in mutated]\n",
    "        off_order = np.argsort(off_int)[::-1]\n",
    "\n",
    "        # 6) Replacement\n",
    "        for k in range(pop_size // 2):\n",
    "            population[order[-(k + 1)]] = mutated[off_order[k]]\n",
    "\n",
    "        # 7) Logging\n",
    "        best_parent_int = intensities[order[0]]\n",
    "        best_off_int = off_int[off_order[0]]\n",
    "        best_int = max(best_parent_int, best_off_int)\n",
    "\n",
    "        if best_int == best_parent_int:\n",
    "            best_mask = population[order[0]]\n",
    "        else:\n",
    "            best_mask = mutated[off_order[0]]\n",
    "\n",
    "        best_int_history.append(best_int)\n",
    "        print(f\"Gen {g:3d}  Best Intensity = {best_int:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'max_intensity': best_int_history,\n",
    "        'best_mask': best_mask\n",
    "    }\n",
    "\n",
    "phi = np.load(\"phase_mask/phi.npy\")\n",
    "results = binary_ga(slm_dim1=16, slm_dim2=16, pop_size=30, generations=200, sigma=0.01, seed=42, phi=phi)\n",
    "print(f\"Final best intensity: {max(results['max_intensity']):.6f}\")\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.plot(results['max_intensity'], label='Best Intensity during Training', color='black')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('Best Intensity Per Generation')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig('results_ga/binary_ga_training_curve.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
